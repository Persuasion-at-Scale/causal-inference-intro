{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Causal Inference: Potential Outcomes and Randomization\n",
    "\n",
    "**Based on Green (2022), *A Primer for Experimental Research on the Effectiveness of Campaign Communication*, Chapters 1-2**\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Persuasion-at-Scale/causal-inference-intro/blob/main/green-causal-inference.ipynb)\n",
    "\n",
    "This notebook walks through four core ideas:\n",
    "1. **Potential outcomes** - the framework for defining causal effects\n",
    "2. **Randomization** - why it produces unbiased estimates\n",
    "3. **Selection bias** - what goes wrong without randomization\n",
    "4. **Experiment analysis** - putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations\n",
    "from scipy import stats\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "BLUE = '#0072B2'\n",
    "ORANGE = '#D55E00'\n",
    "GREEN = '#009E73'\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Potential Outcomes (Green, Table 2.1)\n",
    "\n",
    "Every subject has **two** potential outcomes:\n",
    "- $Y_i(1)$ = outcome if subject $i$ receives treatment\n",
    "- $Y_i(0)$ = outcome if subject $i$ receives control\n",
    "\n",
    "The **individual causal effect** is $\\tau_i = Y_i(1) - Y_i(0)$.\n",
    "\n",
    "The **Average Treatment Effect (ATE)** is the mean of all individual effects:\n",
    "$$\\text{ATE} = \\frac{1}{N} \\sum_{i=1}^{N} \\left[ Y_i(1) - Y_i(0) \\right]$$\n",
    "\n",
    "Green's example: a cash subsidy program that might affect BMI. Five subjects, and (by divine omniscience) we know both potential outcomes for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Green (2022) Table 2.1 - God's-eye view\n",
    "table21 = pd.DataFrame({\n",
    "    'Subject': [1, 2, 3, 4, 5],\n",
    "    'Y(1): BMI if treated': [16, 20, 25, 27, 22],\n",
    "    'Y(0): BMI if control': [11, 12, 25, 29, 18],\n",
    "    'Effect: Y(1)-Y(0)': [5, 8, 0, -2, 4],\n",
    "})\n",
    "table21 = table21.set_index('Subject')\n",
    "\n",
    "print(\"Table 2.1: Potential outcomes for 5 subjects\")\n",
    "print(\"=\"*55)\n",
    "display(table21)\n",
    "\n",
    "true_ate = table21['Effect: Y(1)-Y(0)'].mean()\n",
    "print(f\"\\nTrue ATE = {true_ate:.1f} BMI points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Fundamental Problem of Causal Inference\n",
    "\n",
    "In reality, we **never** see both columns. Each subject is either treated or not. We observe one potential outcome and the other is forever missing - the \"counterfactual.\"\n",
    "\n",
    "Suppose subjects 1 and 2 happen to be treated, and 3, 4, 5 are controls. What does the researcher actually see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What the researcher actually observes\n",
    "Y1 = np.array([16, 20, 25, 27, 22])  # potential outcome if treated\n",
    "Y0 = np.array([11, 12, 25, 29, 18])  # potential outcome if control\n",
    "d  = np.array([1, 1, 0, 0, 0])       # treatment assignment\n",
    "\n",
    "Y_obs = d * Y1 + (1 - d) * Y0  # observed outcomes\n",
    "\n",
    "observed = pd.DataFrame({\n",
    "    'Subject': [1, 2, 3, 4, 5],\n",
    "    'Treated?': ['Yes', 'Yes', 'No', 'No', 'No'],\n",
    "    'Observed BMI': Y_obs.astype(int),\n",
    "    'Y(1)': ['16', '20', '?', '?', '?'],\n",
    "    'Y(0)': ['?', '?', '25', '29', '18'],\n",
    "}).set_index('Subject')\n",
    "\n",
    "display(observed)\n",
    "\n",
    "naive_treated = Y_obs[d == 1].mean()\n",
    "naive_control = Y_obs[d == 0].mean()\n",
    "naive_estimate = naive_treated - naive_control\n",
    "\n",
    "print(f\"Mean BMI (treated):  {naive_treated:.1f}\")\n",
    "print(f\"Mean BMI (control):  {naive_control:.1f}\")\n",
    "print(f\"Naive estimate:      {naive_estimate:.1f}\")\n",
    "print(f\"True ATE:            {true_ate:.1f}\")\n",
    "print(f\"\\nThe naive estimate is OFF by {naive_estimate - true_ate:.1f} points!\")\n",
    "print(\"This is SELECTION BIAS: the treated subjects were different to begin with.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Randomization Fixes Everything\n",
    "\n",
    "With 5 subjects and 2 assigned to treatment, there are $\\binom{5}{2} = 10$ possible random assignments.\n",
    "\n",
    "Green's key claim: the **average** of the difference-in-means across all possible randomizations equals the true ATE. Randomization doesn't guarantee a correct answer for any single experiment, but it's **unbiased** - correct on average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enumerate all 10 possible random assignments\n",
    "subjects = list(range(5))\n",
    "all_assignments = list(combinations(subjects, 2))  # choose 2 for treatment\n",
    "\n",
    "results = []\n",
    "for treated_idx in all_assignments:\n",
    "    d_iter = np.zeros(5, dtype=int)\n",
    "    d_iter[list(treated_idx)] = 1\n",
    "    Y_obs_iter = d_iter * Y1 + (1 - d_iter) * Y0\n",
    "    est = Y_obs_iter[d_iter == 1].mean() - Y_obs_iter[d_iter == 0].mean()\n",
    "    results.append({\n",
    "        'Treated subjects': f\"{treated_idx[0]+1}, {treated_idx[1]+1}\",\n",
    "        'Mean(treated)': Y_obs_iter[d_iter == 1].mean(),\n",
    "        'Mean(control)': Y_obs_iter[d_iter == 0].mean(),\n",
    "        'Estimate': est\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "display(results_df.round(2))\n",
    "\n",
    "avg_estimate = results_df['Estimate'].mean()\n",
    "print(f\"\\nAverage of all 10 estimates: {avg_estimate:.1f}\")\n",
    "print(f\"True ATE:                    {true_ate:.1f}\")\n",
    "print(f\"Match? {np.isclose(avg_estimate, true_ate)}  (unbiasedness!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distribution of estimates\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "estimates = results_df['Estimate'].values\n",
    "colors = [ORANGE if e < 0 else BLUE for e in estimates]\n",
    "\n",
    "ax.scatter(estimates, np.ones(len(estimates)), c=colors, s=200, zorder=3, edgecolors='white', linewidth=1.5)\n",
    "\n",
    "# Jitter overlapping points\n",
    "from collections import Counter\n",
    "counts = Counter(estimates)\n",
    "for val, count in counts.items():\n",
    "    if count > 1:\n",
    "        positions = np.linspace(0.92, 1.08, count)\n",
    "        idxs = [i for i, e in enumerate(estimates) if e == val]\n",
    "        for idx, pos in zip(idxs, positions):\n",
    "            ax.scatter([val], [pos], c=colors[idx], s=200, zorder=3, edgecolors='white', linewidth=1.5)\n",
    "\n",
    "ax.axvline(true_ate, color=GREEN, linewidth=3, label=f'True ATE = {true_ate:.0f}', linestyle='-')\n",
    "ax.axvline(avg_estimate, color='black', linewidth=2, label=f'Mean of estimates = {avg_estimate:.0f}', linestyle='--')\n",
    "\n",
    "ax.set_xlabel('Difference-in-means estimate')\n",
    "ax.set_yticks([])\n",
    "ax.set_title('All 10 possible randomizations (2 treated out of 5 subjects)')\n",
    "ax.legend(fontsize=12)\n",
    "\n",
    "# Label negative estimates\n",
    "ax.annotate('Negative estimates exist!\\n(bad luck of the draw)', xy=(-7, 1.05),\n",
    "            fontsize=10, color=ORANGE, ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Selection Bias: The SAT Prep Example\n",
    "\n",
    "Green Ch 1 uses a running example: do SAT prep courses improve scores?\n",
    "\n",
    "**The problem:** Students who sign up for SAT prep are systematically different from those who don't. They tend to be more motivated, have more resources, and were already better students. So comparing course-takers to non-takers confounds the course effect with pre-existing differences.\n",
    "\n",
    "No matter how many covariates you control for, there's always another unmeasured confounder lurking. Green calls this the \"skeptic's challenge.\"\n",
    "\n",
    "Let's simulate this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate SAT prep self-selection\n",
    "N = 500\n",
    "TRUE_EFFECT = 30  # true effect of SAT prep course (points)\n",
    "\n",
    "# Latent traits\n",
    "ability = np.random.normal(500, 80, N)      # baseline ability\n",
    "motivation = np.random.normal(0, 1, N)       # motivation (unobserved)\n",
    "\n",
    "# Self-selection: higher ability AND motivation -> more likely to take the course\n",
    "propensity = 0.005 * (ability - 500) + 0.8 * motivation\n",
    "p_treat = 1 / (1 + np.exp(-propensity))\n",
    "took_course = np.random.binomial(1, p_treat)\n",
    "\n",
    "# Outcomes: score depends on ability, motivation, AND the course\n",
    "noise = np.random.normal(0, 30, N)\n",
    "score = ability + 20 * motivation + TRUE_EFFECT * took_course + noise\n",
    "\n",
    "sat = pd.DataFrame({\n",
    "    'ability': ability,\n",
    "    'motivation': motivation,\n",
    "    'took_course': took_course,\n",
    "    'score': score\n",
    "})\n",
    "\n",
    "naive_diff = sat.groupby('took_course')['score'].mean()\n",
    "naive_est = naive_diff[1] - naive_diff[0]\n",
    "\n",
    "print(f\"True course effect:  {TRUE_EFFECT} points\")\n",
    "print(f\"Naive estimate:      {naive_est:.1f} points\")\n",
    "print(f\"Bias:                {naive_est - TRUE_EFFECT:.1f} points (selection bias!)\")\n",
    "print(f\"\\nStudents in course:  {took_course.sum()} / {N}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the selection bias\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Panel 1: Ability distributions by group\n",
    "ax = axes[0]\n",
    "ax.hist(sat.loc[sat['took_course']==0, 'ability'], bins=30, alpha=0.6,\n",
    "        color=ORANGE, label='No course', density=True)\n",
    "ax.hist(sat.loc[sat['took_course']==1, 'ability'], bins=30, alpha=0.6,\n",
    "        color=BLUE, label='Took course', density=True)\n",
    "ax.set_xlabel('Baseline ability')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Self-selection: course-takers have higher ability')\n",
    "ax.legend()\n",
    "\n",
    "# Panel 2: Outcomes by group\n",
    "ax = axes[1]\n",
    "groups = sat.groupby('took_course')['score']\n",
    "means = groups.mean()\n",
    "sems = groups.sem()\n",
    "x = [0, 1]\n",
    "bars = ax.bar(x, means, yerr=1.96*sems, capsize=8,\n",
    "              color=[ORANGE, BLUE], width=0.5)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(['No course', 'Took course'])\n",
    "ax.set_ylabel('SAT score')\n",
    "ax.set_title('Outcomes: naive comparison is misleading')\n",
    "\n",
    "# Annotate\n",
    "mid_y = means.mean()\n",
    "ax.annotate(f'Naive diff = {naive_est:.0f}',\n",
    "            xy=(0.5, mid_y), fontsize=13, ha='center', color='red', fontweight='bold')\n",
    "ax.annotate(f'True effect = {TRUE_EFFECT}',\n",
    "            xy=(0.5, mid_y - 30), fontsize=13, ha='center', color=GREEN, fontweight='bold')\n",
    "\n",
    "plt.suptitle('Selection Bias in the SAT Prep Example', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now: randomize treatment on the SAME population\n",
    "random_assignment = np.random.binomial(1, 0.5, N)\n",
    "score_rct = ability + 20 * motivation + TRUE_EFFECT * random_assignment + noise\n",
    "\n",
    "rct = pd.DataFrame({\n",
    "    'ability': ability,\n",
    "    'assigned_course': random_assignment,\n",
    "    'score': score_rct\n",
    "})\n",
    "\n",
    "rct_diff = rct.groupby('assigned_course')['score'].mean()\n",
    "rct_est = rct_diff[1] - rct_diff[0]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Panel 1: Ability distributions (now balanced!)\n",
    "ax = axes[0]\n",
    "ax.hist(rct.loc[rct['assigned_course']==0, 'ability'], bins=30, alpha=0.6,\n",
    "        color=ORANGE, label='Control', density=True)\n",
    "ax.hist(rct.loc[rct['assigned_course']==1, 'ability'], bins=30, alpha=0.6,\n",
    "        color=BLUE, label='Treatment', density=True)\n",
    "ax.set_xlabel('Baseline ability')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Random assignment: groups are balanced')\n",
    "ax.legend()\n",
    "\n",
    "# Panel 2: Outcomes\n",
    "ax = axes[1]\n",
    "means_rct = rct.groupby('assigned_course')['score'].mean()\n",
    "sems_rct = rct.groupby('assigned_course')['score'].sem()\n",
    "bars = ax.bar(x, means_rct, yerr=1.96*sems_rct, capsize=8,\n",
    "              color=[ORANGE, BLUE], width=0.5)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(['Control', 'Treatment'])\n",
    "ax.set_ylabel('SAT score')\n",
    "ax.set_title('Experiment: estimate is close to truth')\n",
    "\n",
    "mid_y_rct = means_rct.mean()\n",
    "ax.annotate(f'RCT estimate = {rct_est:.0f}',\n",
    "            xy=(0.5, mid_y_rct), fontsize=13, ha='center', color=BLUE, fontweight='bold')\n",
    "ax.annotate(f'True effect = {TRUE_EFFECT}',\n",
    "            xy=(0.5, mid_y_rct - 30), fontsize=13, ha='center', color=GREEN, fontweight='bold')\n",
    "\n",
    "plt.suptitle('Randomization Eliminates Selection Bias', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"RCT estimate:    {rct_est:.1f} points\")\n",
    "print(f\"True effect:     {TRUE_EFFECT} points\")\n",
    "print(f\"Bias:            {rct_est - TRUE_EFFECT:.1f} points (sampling noise, not systematic bias)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Running a Simple Experiment\n",
    "\n",
    "Let's simulate a complete experiment and analyze it. We'll:\n",
    "1. Generate a population with known potential outcomes\n",
    "2. Randomly assign treatment\n",
    "3. Compute the difference-in-means, standard error, confidence interval, and p-value\n",
    "4. Repeat 1,000 times to see the sampling distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One experiment\n",
    "N_exp = 200\n",
    "ATE_true = 5.0\n",
    "\n",
    "# Generate potential outcomes\n",
    "Y0_pop = np.random.normal(50, 10, N_exp)\n",
    "Y1_pop = Y0_pop + ATE_true + np.random.normal(0, 3, N_exp)  # heterogeneous effects\n",
    "\n",
    "# Random assignment\n",
    "treated = np.random.binomial(1, 0.5, N_exp)\n",
    "Y_observed = treated * Y1_pop + (1 - treated) * Y0_pop\n",
    "\n",
    "# Analysis\n",
    "y_t = Y_observed[treated == 1]\n",
    "y_c = Y_observed[treated == 0]\n",
    "\n",
    "diff_means = y_t.mean() - y_c.mean()\n",
    "se = np.sqrt(y_t.var(ddof=1)/len(y_t) + y_c.var(ddof=1)/len(y_c))\n",
    "ci_low = diff_means - 1.96 * se\n",
    "ci_high = diff_means + 1.96 * se\n",
    "t_stat, p_val = stats.ttest_ind(y_t, y_c)\n",
    "\n",
    "print(\"Single Experiment Results\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"N treated:     {treated.sum()}\")\n",
    "print(f\"N control:     {N_exp - treated.sum()}\")\n",
    "print(f\"Estimate:      {diff_means:.2f}\")\n",
    "print(f\"Std error:     {se:.2f}\")\n",
    "print(f\"95% CI:        [{ci_low:.2f}, {ci_high:.2f}]\")\n",
    "print(f\"p-value:       {p_val:.4f}\")\n",
    "print(f\"True ATE:      {ATE_true:.1f}\")\n",
    "print(f\"CI covers truth? {ci_low <= ATE_true <= ci_high}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat 1000 times\n",
    "n_sims = 1000\n",
    "estimates = np.zeros(n_sims)\n",
    "ci_lows = np.zeros(n_sims)\n",
    "ci_highs = np.zeros(n_sims)\n",
    "\n",
    "for sim in range(n_sims):\n",
    "    Y0_sim = np.random.normal(50, 10, N_exp)\n",
    "    Y1_sim = Y0_sim + ATE_true + np.random.normal(0, 3, N_exp)\n",
    "    d_sim = np.random.binomial(1, 0.5, N_exp)\n",
    "    Y_sim = d_sim * Y1_sim + (1 - d_sim) * Y0_sim\n",
    "\n",
    "    yt = Y_sim[d_sim == 1]\n",
    "    yc = Y_sim[d_sim == 0]\n",
    "    est = yt.mean() - yc.mean()\n",
    "    se_sim = np.sqrt(yt.var(ddof=1)/len(yt) + yc.var(ddof=1)/len(yc))\n",
    "\n",
    "    estimates[sim] = est\n",
    "    ci_lows[sim] = est - 1.96 * se_sim\n",
    "    ci_highs[sim] = est + 1.96 * se_sim\n",
    "\n",
    "coverage = np.mean((ci_lows <= ATE_true) & (ATE_true <= ci_highs))\n",
    "\n",
    "# Plot sampling distribution\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.hist(estimates, bins=40, color=BLUE, alpha=0.7, edgecolor='white', density=True)\n",
    "ax.axvline(ATE_true, color=GREEN, linewidth=3, label=f'True ATE = {ATE_true}')\n",
    "ax.axvline(estimates.mean(), color='black', linewidth=2, linestyle='--',\n",
    "           label=f'Mean of estimates = {estimates.mean():.2f}')\n",
    "\n",
    "ax.set_xlabel('Difference-in-means estimate')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title(f'Sampling distribution: 1,000 experiments (N={N_exp} each)')\n",
    "ax.legend(fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mean of 1,000 estimates: {estimates.mean():.2f}  (true ATE = {ATE_true})\")\n",
    "print(f\"SD of estimates:         {estimates.std():.2f}\")\n",
    "print(f\"95% CI coverage:         {coverage:.1%}  (should be ~95%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary: Three Assumptions for Unbiased Estimation\n",
    "\n",
    "Green (Ch 2) identifies three core assumptions that, if met, make the difference-in-means an unbiased estimator of the ATE:\n",
    "\n",
    "**1. Random assignment**\n",
    "Treatment is independent of potential outcomes. Each subject had the same probability of being treated, determined by chance rather than choice. Threats: tampering, differential attrition.\n",
    "\n",
    "**2. Noninterference (SUTVA)**\n",
    "My outcomes depend only on MY treatment status, not on whether others are treated. Threats: spillover effects, competition for limited resources, social interaction between treatment and control groups.\n",
    "\n",
    "**3. Symmetry**\n",
    "Treatment is the ONLY systematic difference between groups. The treated group didn't also get extra attention, different measurement procedures, or a compound bundle of changes. Threats: Hawthorne effects, compound treatments, differential measurement."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}